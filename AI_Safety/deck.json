{
    "__type__": "Deck",
    "children": [],
    "crowdanki_uuid": "03ab7ac6-4679-11ed-94b6-00248c528cf7",
    "deck_config_uuid": "03aba1f4-4679-11ed-bd28-00248c528cf7",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoplay": true,
            "buryInterdayLearning": false,
            "crowdanki_uuid": "03aba1f4-4679-11ed-bd28-00248c528cf7",
            "dyn": false,
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 1,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "Default",
            "new": {
                "bury": false,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 40
            },
            "newGatherPriority": 0,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "replayq": true,
            "rev": {
                "bury": false,
                "ease4": 1.3,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "perDay": 200
            },
            "reviewOrder": 0,
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [],
    "name": "AI Safety",
    "newLimit": null,
    "newLimitToday": null,
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "css": ".card {\n    font-family: arial;\n    font-size: 20px;\n    text-align: center;\n    color: black;\n    background-color: white;\n}\n",
            "flds": [
                {
                    "description": "",
                    "font": "Arial",
                    "name": "Front",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "description": "",
                    "font": "Arial",
                    "name": "Back",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Basic",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Back}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "name": "Card 1",
                    "ord": 0,
                    "qfmt": "{{Front}}"
                }
            ],
            "type": 0
        }
    ],
    "notes": [
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Aligned AI (2022)",
                "<div><div><span><span><span><a href=\"https://forum.effectivealtruism.org/posts/emKDqNjyE2h22MJ2T/we-re-aligned-ai-we-re-aiming-to-align-ai\"><u>Aligned AI</u></a></span></span></span>&nbsp;/ Stuart Armstrong</div></div><div><span><span><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1\"><u>Model splintering</u></a></span></span> <br></div><div>Stuart's plan to solve this problem is as follows:<ol><li>Maintain a set of all possible extrapolations of reward data that are consistent with the training process</li><li>Pick among these for a safe reward extrapolation.&nbsp;</li></ol><p>They are currently working on algorithms to accomplish step 1: see <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/i8sHdLyGQeBTGwTqq/value-extrapolation-concept-extrapolation-model-splintering\"><u>Value Extrapolation</u></a></span></span>. <br></p><p><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></p></div>"
            ],
            "guid": "zrlUU03yJg",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Model Splintering?<br>",
                "\"this approach seems safe in this imperfect model, but when we generalize the model more, it becomes dangerously underdefined\"<div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\"><span><span></span></span></a><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1\">Model splintering</a></div>"
            ],
            "guid": "jOJTafx/ag",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Alignment Research Center (ARC) (2022)",
                "<div><div><span><span><span><a href=\"https://alignment.org/\"><u>Alignment Research Center (ARC)</u></a></span></span></span><span><span><span><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\"><u>Eliciting Latent Knowledge</u></a></span></span></span>&nbsp;/ Paul Christiano</div><div>Working on interpretability through using a reporter to interpret and report on a predictor<span><span><span></span></span></span><span><span><br></span></span></div><div><span><span><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/e3j7h4mPHvkRynbco/help-arc-evaluate-capabilities-of-current-language-models\"><u>Evaluating LM power-seeking</u></a></span></span>&nbsp;/ Beth Barnes</div></div>Generating a dataset that we can use to evaluate how close models are to being able to successfully seek power<div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "j|=v&f*(<&",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Eliciting Latent Knowledge?<br>",
                "<div><span><span><span><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\"><u>Eliciting Latent Knowledge (ELK)</u></a></span></span></span><span><span><span></span></span></span></div><div><br></div><div>Finding knowledge that is known to a model that it is not necessarily reporting - ARC w/ Christiano is using a reporter to report on a predictor to solve interpretability</div>"
            ],
            "guid": "dn66Jjd3fz",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - NAH<br>",
                " <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro\"><u>The Natural Abstraction Hypothesis</u></a></span></span><br>"
            ],
            "guid": "KCC1(PzuVu",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Jan Leike (2022)<br>",
                "<div>Employment:</div><div><span><span><span><a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>OpenAI</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span>minimum viable product <a href=\"https://aligned.substack.com/p/alignment-mvp\"><u>aligned AGI</u></a></span></span></span></div><div><span><span><span>Interpretability - neuron monosemanticity<br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span><a href=\"https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35\"><u>the informed oversight problem</u></a></span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "AL&vhu53_q",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Anthropic (2022)",
                "<div><span><span><span><a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a></span></span></span> </div><div><br></div><div>LLM Alignment</div><div>Fine tuning a LM to be more <span><span><span><a href=\"https://arxiv.org/abs/2112.00861\"><u>Helpful, Honest, and Harmless (HHH)</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div>Interpretability (Chris Olah)</div><div>Decompiling networks into better representations that is more interpretable, by increasing neuron monosemanticity (making a neuron represent only one thing, as opposed to firing on many unrelated concepts)</div><div><br></div><div>Scaling Laws</div><div>Model performance scaling to predict how future AI might look</div><div><i>data + params + compute = performance</i></div><div>Increasing <span><span><span><a href=\"https://arxiv.org/abs/2001.08361\"><u>data, parameters, and compute all at the same time</u></a></span></span></span><br></div><div><i><br></i></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "OmKL~4c^h2",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Brain-Like-AGI Safety (2022)",
                "<span><span><a class=\"SequencePreview-link\" id=\"HzcM2dkCq7fwXBej8\" href=\"https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8\"><u>Brain-Like-AGI Safety<br></u></a></span></span><div>Steven Byrnes</div><div><br></div><div>How would we align an AGI whose learning algorithms / cognition look like human brains?</div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "lIsT{sk9KZ",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is X Risk?<br>",
                "Existential Risk - risk of an extermination-level event for humans or sentient life<br>"
            ],
            "guid": "NF`DF5;Q(*",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is S Risk?<br>",
                "S risk is Suffering Risk: the risk that the future holds extreme levels of suffering<br>"
            ],
            "guid": "FpSa9yK`B*",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Center for AI Safety (CAIS) (2022)",
                "<div><span><span><span><a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a></span></span></span>&nbsp;(CAIS)</div><div>Dan Hendrycks</div><div><br></div><p>CAIS is working on a number of projects, including:</p><ul><li>Writing papers that <span><span><span><a href=\"https://arxiv.org/abs/2206.05862\"><u>talk about x-risk</u></a></span></span></span>.</li><li>Publishing compilations of <span><span><span><a href=\"https://arxiv.org/abs/2109.13916\"><u>open problems</u></a></span></span></span>.</li><li>Make safety benchmarks that the ML community can iterate on.</li><li>Running a <span><span><span><a href=\"https://safe.ai/competitions\"><u>NeurIPS competition</u></a></span></span></span>&nbsp;on these benchmarks.</li><li>Running the ML Safety Scholars program (<span><a>MLSS</a></span>)</li><li>A <span><span><span><a href=\"https://philosophy.safe.ai/\">Philosophy Fellowship</a></span></span></span> aimed at recruiting philosophers to do conceptual alignment research.&nbsp;</li></ul>Trojan detection<br><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "y=6:m#q6>i",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Cooperative Inverse Reinforcement Learning?<br>",
                "<div><span><span><span><a href=\"https://arxiv.org/abs/1606.03137\"><u>CIRL</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div>A cooperative, partial-information game with two agents, human and robot; both\nare rewarded according to the human's reward function, but the robot does not\ninitially know what this is</div><div>Induces active learning and foregoes the assumption that the human acts perfectly to maximize its reward at all timesteps.</div>"
            ],
            "guid": "qQ^1y~}~Kc",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Center for Human Compatible AI (CHAI) (2022)",
                "<div><span><span><span><a href=\"https://humancompatible.ai/\"><u>Center for Human Compatible AI (CHAI)</u></a></span></span></span><br></div><div>Stuart Russell - UC Berkeley<br></div><div><br></div><div>Cooperative Inverse Reinforcement Learning (CIRL)</div><div>Clusterability in Neural Networks - Modularity</div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "kU9,Q+o7P~",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "<div>Agenda - Center on Long Term Risk (CLR) (2022)</div>",
                "<div><div><span><span><span><a href=\"https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf\"><u>Center on Long Term Risk (CLR)</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div></div><div>Reducing S-Risk<span><span><span><br></span></span></span></div>Game theory / Decision Theory<div>Multipolar AI scenarios</div><div>Works with <span><span><span><a href=\"https://www.cooperativeai.com/foundation\">The Cooperative AI Foundation (<u>CAIF)</u></a></span></span></span><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div><a href=\"https://www.cooperativeai.com/foundation\"></a></div>"
            ],
            "guid": "Pak+8j2ThA",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Conjecture (2022)",
                "<div><div><span><span><span><a href=\"https://www.conjecture.dev/\"><u>Conjecture's website</u></a></span></span></span> <br></div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/jfq2BH5kfQqu2vYv3/we-are-conjecture-a-new-alignment-research-startup#Our_Research_Agenda\"><u>Who is Conjecture?</u></a><br></span></span></div><div><br></div><div>focused on aligning LLMs</div><div>has short timelines &amp; alignment is hard</div><div>averse to infohazards</div><div>Scalable LLM Interpretability<span><span><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/D7epkkJb3CqDTYgX9/refine-an-incubator-for-conceptual-alignment-research-bets\"><u>Refine</u></a></span></span>: incubator for decorrelated alignment research bets</div><div>Simulacra Theory - making a non-agentic AI to help with alignment research<br><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "As=hbQ0~D(",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about David Krueger (2022)<br>",
                "<div>Employment:</div><div>University of Cambridge</div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span>Runs a lab at University of Cambridge<br></span></span></span></div><div><span><span><span>Inner Alignment - operationalizing inner alignment failures</span></span></span> <span><span><span><a href=\"https://arxiv.org/abs/2105.14111\"><u>Goal Misgeneralization</u></a></span></span></span></div><div><span><span><span>Neural network generalization </span></span></span><span><span><span><a href=\"http://proceedings.mlr.press/v139/krueger21a.html\"><u>OOD Generalization via Risk Extrapolation</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "g0SCaYlLxK",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - DeepMind (2022)",
                "<div><div><span><span><span><a href=\"https://www.deepmind.com/\"><u>DeepMind</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div></div><div>has ML safety team focused on near-term risks<span><span><span><br></span></span></span></div>has Alignment team working on AGI risks<div><p>Some of the work they are doing is:&nbsp;</p><ul><li><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments\"><u>Engaging with recent MIRI arguments</u></a></span></span>.&nbsp;</li><li>Rohin Shah produces the <span><span><span><a href=\"https://rohinshah.com/alignment-newsletter/\"><u>alignment newsletter</u></a></span></span></span>.</li><li>Publishing interesting research like the <span><span><span><a href=\"https://arxiv.org/abs/2105.14111\"><u>Goal Misgeneralization paper</u></a></span></span></span>.&nbsp;</li><li>Geoffrey Irving is working on debate as an alignment strategy: more <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/bLr68nrLSwgzqLpzu/axrp-episode-16-preparing-for-debate-ai-with-geoffrey-irving\"><u>detail here</u></a></span></span>.</li><li><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents\">Discovering agents</a></span></span>, which introduces a causal definition of agents, then introduces an algorithm for finding agents from empirical data.&nbsp;</li></ul><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">Source</a></div>"
            ],
            "guid": "Ce[@v@3(Z.",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Dylan Hadfield-Menell (2022)<br>",
                "<div><div>Employment:</div><div><a href=\"https://www.anthropic.com/\"><span><span><span></span></span></span></a><a href=\"http://people.csail.mit.edu/dhm/\"><u>Dylan Hadfield-Menell</u></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span><a href=\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-207.pdf\"><u>Dylan's PhD thesis</u></a></span></span></span></div><div><ol><li>Outer alignment failures are a problem.</li><li>We can mitigate this problem by adding in uncertainty.</li><li>We can model this as&nbsp;<span><span><span><a href=\"https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html\"><u>Cooperative Inverse Reinforcement Learning (CIRL)</u></a></span></span></span>.&nbsp; <br></li></ol><span><span><span></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div></div>"
            ],
            "guid": "dQ[l&u*NH^",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Encultured (2022)<br>",
                "<span><span><span><a href=\"https://www.encultured.ai/\"><u>Encultured</u></a></span></span></span><div><p>See <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/ALkH4o53ofm862vxc/announcing-encultured-ai-building-a-video-game\"><u>post</u></a></span></span>.</p><p>Andrew Critch<br></p></div><div>multiplayer video games as test environment for AI</div><div>concerned about multipolar AI scenarios<br><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "*V=Kn^pjs",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Externalized Reasoning Oversight (2022)<br>",
                "<div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for\"><u>Externalized Reasoning Oversight</u></a><br></span></span></div><div><div>Tamera Lanham<span><span><br></span></span></div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for\"><u></u></a></span></span></div><div><br></div><div>Make an AGI as an LLM externalize its reasoning<br><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "j|85-kakIJ",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Future of Humanity Institute (FHI) (2022)<br>",
                "<div><span><span><span><a href=\"https://www.fhi.ox.ac.uk/\"><u>Future of Humanity Institute</u></a></span></span></span><br></div><div><br></div><div>the <span><span><span><a href=\"https://www.fhi.ox.ac.uk/causal-incentives-working-group/\"><u>Causal incentives group</u></a></span></span></span> (joint between FHI and DeepMind)</div><div>reward tampering<br><div><br><ul><li><span><span><span><a href=\"https://arxiv.org/abs/2102.01685\">Agent Incentives: A Causal Perspective</a></span></span></span>, a paper which formalizes concepts such as the value of information and control incentives.&nbsp;</li><li><span><span><span><a href=\"https://arxiv.org/abs/1908.04734\">Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective</a></span></span></span>, a paper which theoretically analyzes wireheading.&nbsp;</li></ul></div><div> </div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "J]Nq,etaN/",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Fund For Alignment Research (FAR) (2022)<br>",
                "<div><div><span><span><span><a href=\"https://alignmentfund.org/\">Fund For Alignment Research (FAR)</a><br></span></span></span></div><div><span><span><span><br></span></span></span></div></div><div>create new and scalable alignment agendas<span><span><span><br></span></span></span></div>RLHF and interpretability<div>search broadly and double down on promising approaches</div><div>The <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool#:~:text=TL%3BDR%3A%20We're,(%E2%80%9Cinverse%20scaling%E2%80%9D).\">inverse scaling law</a></span></span> prize.<div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "fNP(]h,o8",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Machine Intelligence Research Institute (MIRI) (2022)<br>",
                "<div><span><span><span><a href=\"https://intelligence.org/\"><u>MIRI</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem\"><u>Deception + Inner Alignment</u></a></span></span>&nbsp;/ Evan Hubinger<span><span><br></span></span></div><div><span><span><a class=\"TagHoverPreview-link\" href=\"https://www.lesswrong.com/tag/agent-foundations\"><u>Agent Foundations</u></a></span></span>&nbsp;/ Scott Garrabrant and Abram Demski<span><span><br></span></span></div><div><span><span><a class=\"SequencePreview-link\" id=\"CmrW8fCmSLK7E25sa\" href=\"https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa\"><u>Infra-Bayesianism</u></a></span></span>&nbsp;/ Vanessa Koso<span><span>y<br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement\"><u>Visible Thoughts Project</u></a><br></span></span></div><div><span><span><br></span></span></div><div>Alignment is really hard</div><div>We must provide a mathematically formal solution if we want to succeed in alignment<div><br></div><div>(INCOMPLETE)<br></div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "J.oTOogW3X",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Deception + Inner Alignment (in MIRI) (2022)",
                "<div>Evan Hubinger</div><div><br></div><div>Deceptive agents are the default</div><div>Possible solution is to make sure agents are <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia\"><u>myopic</u></a></span></span> - “return the action that your model of <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch\">HCH</a></span></span> would return, if it received your inputs.”<span><span></span></span><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/LCLBnmwdxkkz5fNvH/open-problems-with-myopia\"></a></div>"
            ],
            "guid": "HE}N!UM`mC",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Human Consulting Human (HCH)?",
                " <div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch\">HCH</a> - Human Consulting Human</span></span></div><div><span><span><br></span></span></div><div><p>Consider a human Hugh who has access to a question-answering machine.\n Suppose the machine answers question Q by perfectly imitating how Hugh \nwould answer question Q, <em>if</em> <em>Hugh had access to the question-answering machine</em>.</p><p>That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh…</p></div><div><span><span></span></span></div>"
            ],
            "guid": "A3%O5F=<L%",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Agent Foundations (in MIRI) (2022)",
                "<div>Scott Garrabrant &amp; Abram Demski</div><div><br></div><div>They are working on fundamental problems like <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><u>embeddedness, decision theory, logical counterfactuals</u></a></span></span>, and more. <br></div><div>A big advance was <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames\"><u>Cartesian Frames</u></a></span></span>, a formal model of agency.</div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "I=717Oq=f<",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is The Informed Oversight Problem?<br>",
                "<div><span><span><span><a href=\"https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35\"><u>the informed oversight problem</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div>"
            ],
            "guid": "IfGX>K*_R#",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Infra-Bayesianism (in MIRI) (2022)<br>",
                "<div><div><span><span><a class=\"SequencePreview-link\" id=\"CmrW8fCmSLK7E25sa\" href=\"https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa\"><u>Infra-Bayesianism</u></a></span></span></div></div><div>Vanessa Kosoy<span><span><br></span></span></div><div>Vanessa's<span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda\"> research agenda</a></span></span></div><div><span><span><br></span></span></div>approach alignment from a mathematically formal position:<ol><li>Find all of the agents that preceded the AI</li><li>Discard all of these agents that are powerful / non-human like</li><li>Find all of the utility functions in the remaining agents</li><li>Use combination of all of these utilities as the agent's utility function</li></ol><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div>"
            ],
            "guid": "AXF9Eb-1Q|",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Visible Thoughts Project (in MIRI) (2022)",
                "<div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement\"><u>Visible Thoughts Project</u></a></span></span></div><div><br></div><div>Train a LLM to make it show what characters are thinking - hopefully making its own reasoning interpretable<br></div><div><br></div><div>https://www.lesswrong.com/posts/zRn6cLtxyNodudzhw/visible-thoughts-project-and-bounty-announcement</div><div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "eG>8~xoyhF",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Jacob Steinhardt (2022)<br>",
                "<div><div>Employment:</div><div>UC Berkeley</div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><div>robustness &amp; distribution shift</div><div>technical paper he wrote is&nbsp;<span><span><span><a href=\"https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LKv32bgAAAAJ&amp;citation_for_view=LKv32bgAAAAJ:4TOpqqG69KYC\"><u>Certified defenses against adversarial examples</u></a><br></span></span></span></div>He's published several technical overviews including&nbsp;<span><span><span><a href=\"https://docs.google.com/document/d/1FbTuRvC4TFWzGYerTKpBU7FJlyvjeOvVYF2uYNFSlOc/edit#\"><u>AI Alignment Research Overview</u></a></span></span></span> and&nbsp;<span><span><span><a href=\"https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=LKv32bgAAAAJ&amp;citation_for_view=LKv32bgAAAAJ:kNdYIx-mwKoC\"><u>Concrete problems in AI safety</u></a></span></span></span>, and created an&nbsp;<span><span><span><a href=\"https://bounded-regret.ghost.io/ai-forecasting/\"><u>AI forecasting competition</u></a></span></span></span>. </div><div>Runs blog:&nbsp;<a href=\"https://bounded-regret.ghost.io/author/jacob/\">Bounded Regret</a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><br><span><span><span></span></span></span></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "K!{dP&z>&J",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Reinforcement Learning from Human Feedback (RLHF)?",
                "<div>Reinforcement Learning from Human Feedback</div><div>INCOMPLETE</div>"
            ],
            "guid": "Lgo+YXF(Be",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - OpenAI (2022)<br>",
                "<div><div><span><span><span><a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>OpenAI</u></a><br></span></span></span></div></div><div>Jan Leike<span><span><span><br></span></span></span></div><div><br></div><div>build a minimal AGI that is aligned to help us with the full alignment problem</div><div>Reinforcement Learning from Human Feedback (RLHF)</div><div><div><span><span><span><a href=\"https://ai-alignment.com/the-informed-oversight-problem-1b51b4f66b35\"><u>the informed oversight problem</u></a></span></span></span> &amp; an <span><span><span><a href=\"https://aligned.substack.com/p/ai-assisted-human-feedback\"><u>AI assisted oversight scheme</u></a></span></span></span></div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "bEHm/gIuO",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Ought (2022)<br>",
                "<div><div><span><span><span><a href=\"https://ought.org/\"><u>Ought</u></a></span></span></span></div></div><div><br></div><div>Advancing <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes\"><u>process-based systems</u></a></span></span></div><div>Elicit - an AI research assistant<span><span><span><br></span></span></span></div>scale open-ended reasoning<div>process-based rather than outcome-based</div><div>improving reasoning will help with alignment<br><div><br></div><div>Ought’s impact on AI alignment has 2 components: (a) improved reasoning of AI governance &amp; alignment researchers, <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Differential_capabilities__Supervising_process_helps_with_long_horizon_tasks\"><u>particularly on long-horizon tasks</u></a></span></span>&nbsp;and (b) <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pYcFPMBtQveAjcSfH/supervise-process-not-outcomes#Alignment__Supervising_process_is_safety_by_construction\"><u>pushing supervision of process rather than outcomes</u></a></span></span>, which reduces the optimization pressure on imperfect proxy objectives leading to “safety by construction”. </div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "O3wN/g?mZL",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Redwood Research (2022)<br>",
                "<div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project\"><u>Adversarial training</u></a> - the plan if we had to align AGI right now<br></span></span></div><div>Addressing nearest term AGI conception: create extremely reliable AI where it will never engage in certain types of behavior<span><span><span><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span><a href=\"https://youtu.be/PDvAutARum4?t=1440\">LLM interpretability</a> - </span></span></span> <span><span><span><a href=\"http://interp-tools.redwoodresearch.org/#/?interpsite=%7B%22whichModel%22%3A%22attention_only_two_layers%22%2C%22prompt%22%3A%22Welcome+to+the+Redwood+Interpretability+Website%21+Click+%5C%22How+to+Use%5C%22+for+documentation.+If+you+have+feedback%2C+add+comments+to+the+doc+or+contact+tao+at+rdwrs+dot+com.+The+Redwood+Interpretability+team+wishes+you+the+best+in+your+interpretability+endeavors%21%22%2C%22nonce%22%3A1%2C%22allNonce%22%3A8%2C%22whichAttributionUI%22%3A%22tree%22%2C%22diagramsUI%22%3A%7B%22attributionSource%22%3A%7B%22kind%22%3A%22logprob%22%2C%22data%22%3A%7B%22seqIdx%22%3A5%2C%22tokString%22%3A%22+red%22%2C%22comparisonTokString%22%3Anull%7D%7D%7D%2C%22indirectUI%22%3A%7B%22maskSchema%22%3A%22true%22%2C%22lossToken%22%3A%22+the%22%2C%22lossComparisonToken%22%3Anull%2C%22seqPos%22%3A%22-2%22%2C%22lossFnKind%22%3A%22logit%22%7D%2C%22showComposable%22%3Atrue%2C%22showAttribution%22%3Atrue%2C%22showDiagrams%22%3Afalse%2C%22showIndirect%22%3Afalse%2C%22composableUI%22%3A%7B%22lvntDims%22%3A%5B%5B57%2C57%5D%2C%5B57%2C490%5D%2C%5B2%2C8%2C57%2C57%5D%2C%5B512%2C256%5D%2C%5B57%2C57%5D%2C%5B2%2C50258%2C256%5D%2C%5B2%2C8%2C57%2C256%5D%2C%5B2%2C1%2C57%2C256%5D%2C%5B2%2C2%2C57%2C57%5D%2C%5B2%2C8%2C57%2C57%5D%2C%5B2%2C8%2C2%2C57%2C32%5D%2C%5B57%5D%2C%5B2%2C8%2C57%2C57%5D%2C%5B2%2C8%2C57%2C57%5D%2C%5B4%2C2%2C8%2C32%2C256%5D%5D%2C%22nonce%22%3A1%2C%22panels%22%3A%5B%7B%22vizName%22%3A%22TextAverageTo%22%2C%22spec%22%3A%5B%22axis%22%2C%22axis%22%5D%2C%22focus%22%3A%5Bnull%2Cnull%5D%2C%22options%22%3A%7B%7D%2C%22lvntIdx%22%3A0%2C%22hover%22%3A%5Bnull%2Cnull%5D%2C%22hideConfig%22%3Afalse%2C%22axisPermutationToShow%22%3A%5B0%2C1%5D%2C%22lvntOptions%22%3A%7B%22QKV%22%3A0%2C%22Attention+Probs%22%3A0%2C%22MLPs%22%3A0%7D%7D%5D%7D%2C%22attributionUI%22%3A%7B%22tree%22%3A%5B%5D%2C%22root%22%3A%7B%22kind%22%3A%22logprob%22%2C%22threshold%22%3A0.1%2C%22data%22%3A%7B%22seqIdx%22%3A55%2C%22tokString%22%3A%22%21%22%2C%22comparisonTokString%22%3Anull%7D%7D%2C%22lineWidthScale%22%3A1%2C%22useIGAttn%22%3Afalse%2C%22useIGOutput%22%3Atrue%2C%22showNegative%22%3Atrue%2C%22useActivationVsMean%22%3Afalse%2C%22fuseNeurons%22%3Atrue%2C%22fakeMlp%22%3A%22none%22%2C%22threshold%22%3A0.1%2C%22specificLogits%22%3A%5B%5D%2C%22modelName%22%3A%22attention_only_two_layers%22%2C%22nonce%22%3A0%2C%22toks%22%3A%5B%22%5BBEGIN%5D%22%2C%22+Welcome%22%2C%22+to%22%2C%22+the%22%2C%22+Red%22%2C%22wood%22%2C%22+Interpret%22%2C%22ability%22%2C%22+Website%22%2C%22%21%22%2C%22+Click%22%2C%22+%5C%22%22%2C%22How%22%2C%22+to%22%2C%22+Use%22%2C%22%5C%22%22%2C%22+for%22%2C%22+documentation%22%2C%22.%22%2C%22+If%22%2C%22+you%22%2C%22+have%22%2C%22+feedback%22%2C%22%2C%22%2C%22+add%22%2C%22+comments%22%2C%22+to%22%2C%22+the%22%2C%22+doc%22%2C%22+or%22%2C%22+contact%22%2C%22+t%22%2C%22ao%22%2C%22+at%22%2C%22+r%22%2C%22d%22%2C%22w%22%2C%22rs%22%2C%22+dot%22%2C%22+com%22%2C%22.%22%2C%22+The%22%2C%22+Red%22%2C%22wood%22%2C%22+Interpret%22%2C%22ability%22%2C%22+team%22%2C%22+wishes%22%2C%22+you%22%2C%22+the%22%2C%22+best%22%2C%22+in%22%2C%22+your%22%2C%22+interpret%22%2C%22ability%22%2C%22+endeavors%22%2C%22%21%22%5D%2C%22onlyOV%22%3Afalse%7D%7D\">website for visualizing transformers</a></span></span></span></div><div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "bN-$e}QQr&",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Sam Bowman (2022)<br>",
                "<div><div>Employment:<span><span><span><br></span></span></span></div><div><span><span><span><a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a></span></span></span> (temporarily)<br></div><div>Sam runs a lab at NYU</div><br><div><span><span><span>Working on:</span></span></span><span><span><span><br></span></span></span></div><div><span><span><span><a href=\"https://cims.nyu.edu/~sbowman/\"><u>His agenda <br></u></a></span></span></span></div><div>language model alignment by&nbsp;<span><span><span><a href=\"https://arxiv.org/abs/2110.08193\"><u>creating</u></a></span></span></span>&nbsp;<span><span><span><a href=\"https://arxiv.org/abs/2206.04615\"><u>datasets</u></a></span></span></span> for evaluating language models</div><div>inductive biases of LLMs</div><div>running&nbsp;<span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool#:~:text=TL%3BDR%3A%20We're,(%E2%80%9Cinverse%20scaling%E2%80%9D).\"><u>the inverse scaling prize</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span></span></span></span><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "b9M)r?vb,e",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Selection Theorems (2022)<br>",
                "<div>John Wentworth</div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents\"><u>selection theorems</u></a></span></span>: try to figure out what types of agents are selected for in a broad range of environments</div><div>Two key properties that might be selected for are modularity and abstractions<div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div></div>"
            ],
            "guid": "n+p.l6X[tR",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Natural Abstractions Hypothesis???<br>",
                "<div> <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro\"><u>The Natural Abstraction Hypothesis</u></a></span></span>&nbsp;(NAH)</div><div><br></div><div> a wide variety of cognitive architectures will tend to use similar abstractions to reason about the world</div>"
            ],
            "guid": "H84,`/L(a^",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Team Shard (2022)<br>",
                "<div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview\"><u>Team Shard</u></a></span></span></div> <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\"><u></u></a></span></span></div><div><br></div><div>make an AI that imitates that circuitry in human brains<span><span><br></span></span></div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\"><u>Reward is not the optimization target</u></a></span></span><div><div><br></div><div>INCOMPLETE<br></div><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target\"></a></div>"
            ],
            "guid": "$|&D%%BWk",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Agenda - Truthful AI (2022)<br>",
                "<div><div><span><span><span><a href=\"https://truthful.ai/\"><u>Truthful AI</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div></div><div>Owain Evans / Owen Cotton-Barratt<span><span><span><br></span></span></span></div>stop models from lying<div> <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi\"><u>Truthful LMs as a warm-up for aligned AGI</u></a></span></span>&nbsp;and <span><span><span><a href=\"https://arxiv.org/abs/2109.07958\"><u>TruthfulQA: Measuring How Models Mimic Human Falsehoods</u></a></span></span></span><div><br></div><div><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/QBAjndPuFbhEXKcCr/my-understanding-of-what-everyone-in-technical-alignment-is#Thomas_s_Alignment_Big_Picture\">source</a></div><a href=\"https://arxiv.org/abs/2109.07958\"></a></div>"
            ],
            "guid": "m<^9?3|g<B",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "agenda"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Chris Olah (2022)",
                "<div>Employment:</div><div><span><span><span><a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span>Interpretability - neuron monosemanticity<br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>(needs source)<br></span></span></span></div>"
            ],
            "guid": "][YI*r-Na",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Paul Christiano (2022)",
                "<div>Employment:</div><div><div><span><span><span><a href=\"https://alignment.org/\"><u>Alignment Research Center (ARC)</u></a></span></span></span><span><span><span><br></span></span></span></div><a href=\"https://www.anthropic.com/\"></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span>Interpretability - </span></span></span><span><span><span><u><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">Eliciting Latent Knowledge (ELK)</a></u></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div>- \"Powerful AI systems have a good chance of deliberately and irreversibly disempowering humanity.\"&nbsp;<a href=\"https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer\">Source</a></div><div>- It's a good idea to focus on sub problems first. One of the things worth pursuing is \"low-stakes\" situations first.&nbsp;<a href=\"https://www.alignmentforum.org/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment\">source</a></div><div>- - A \"good\" objective is similar to finding a \"good\" specification for adversarial training</div><div>- - finding a good objective likely requires ELK</div>"
            ],
            "guid": "nLg*sh_Gav",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Beth Barnes (2022)<br>",
                "<div>Employment (2022):</div><div><div><span><span><span><a href=\"https://alignment.org/\"><u>Alignment Research Center (ARC)</u></a></span></span></span><span><span><span><br></span></span></span></div><a href=\"https://www.anthropic.com/\"></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/e3j7h4mPHvkRynbco/help-arc-evaluate-capabilities-of-current-language-models\"><u>Evaluating LM power-seeking</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "q+v}210S3(",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CIRL<br>",
                "<span><span><span><a href=\"https://arxiv.org/abs/1606.03137\"><u>Cooperative Inverse Reinforcement Learning</u></a></span></span></span>"
            ],
            "guid": "CwgQQ{4VpJ",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Dan Hendrycks (2022)<br>",
                "<div>Employment:</div><div><a href=\"https://www.anthropic.com/\"><span><span><span></span></span></span></a><a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a>&nbsp;(CAIS)</div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><span>- broadening AI safety community<br></span></span></span></div><div><span><span><span>- Trojan Detection</span></span></span></div><div><span><span><span>- scaling laws</span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>- AI safety community should pursue broad research directions at once&nbsp;</span></span></span><a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/dfRtxWcFDupfWpLQo\">source</a>&nbsp;</div><div>- Strongly against capabiliities externalities&nbsp;<a href=\"https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt/p/dfRtxWcFDupfWpLQo\">source</a>&nbsp;</div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "g{8r?@(pVk",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Stuart Russell (2022)<br>",
                "<div>Employment:</div><div><a href=\"https://www.anthropic.com/\"><span><span><span></span></span></span></a><a href=\"https://humancompatible.ai/\"><u>Center for Human Compatible AI (CHAI)</u></a></div><span><span><span>UC Berkeley</span></span></span><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div>Alignment - cooperative inverse reinforcement learning (<span><span><span><a href=\"https://arxiv.org/abs/1606.03137\"><u>CIRL</u></a></span></span></span>)</div><div> <span><span><span><a href=\"https://arxiv.org/abs/2103.03386\"><u>Clusterability in neural networks</u></a></span></span></span> - modularity<br></div><div><br></div><div><span><span><span>Views:</span></span></span></div><div>wrote the book <span><span><span><a href=\"https://en.wikipedia.org/wiki/Human_Compatible\"><u>Human Compatible</u></a></span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "D>Q:%#0u%V",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CHAI<br>",
                "<span><span><span><a href=\"https://humancompatible.ai/\"><u>Center for Human Compatible AI<br></u></a></span></span></span>"
            ],
            "guid": "hYRC1*ncQ-",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CAIS<br>",
                "<span><span><span><a href=\"https://safe.ai/\"><u>Center for AI Safety</u></a></span></span></span>"
            ],
            "guid": "E1lpI;OSD%",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CLR<br>",
                "<div><span><span><span><a href=\"https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf\"><u>Center on Long Term Risk<br></u></a></span></span></span></div>"
            ],
            "guid": "E.U:K+Qbs}",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CAIF<br>",
                "<span><span><span><a href=\"https://www.cooperativeai.com/foundation\">The Cooperative AI Foundation<br></a></span></span></span>"
            ],
            "guid": "PE,5(3XN&M",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - LLM<br>",
                "<div>Large Language Model</div><div>(alternative - Logic Learning Machine)<br></div>"
            ],
            "guid": "fO?8^+nV>f",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - ARC<br>",
                "<div><span><span><span><a href=\"https://alignment.org/\"><u>Alignment Research Center</u></a></span></span></span><span><span><span><br></span></span></span></div>"
            ],
            "guid": "p$i+g[~<dj",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - ELK<br>",
                "<span><span><span><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\"><u>Eliciting Latent Knowledge</u></a></span></span></span>"
            ],
            "guid": "zd,4xl,_r5",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - HHH<br>",
                "<span><span><span><a href=\"https://arxiv.org/abs/2112.00861\"><u>Helpful, Honest, and Harmless</u></a></span></span></span>"
            ],
            "guid": "jB1^%v3qDq",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Scott Garrabrant (2022)<br>",
                "<div>Employment:<span><span><br></span></span></div><div><span><span><a class=\"TagHoverPreview-link\" href=\"https://www.lesswrong.com/tag/agent-foundations\"><u>Agent Foundations</u></a><u> (MIRI)<br></u></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><u>embeddedness, decision theory, logical counterfactuals</u></a><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames\"><u>Cartesian Frames</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "l[05vP+eY|",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Abram Demski (2022)<br>",
                "<div>Employment:<span><span><br></span></span></div><div><span><span><a class=\"TagHoverPreview-link\" href=\"https://www.lesswrong.com/tag/agent-foundations\"><u>Agent Foundations</u></a><u> (MIRI)<br></u></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version\"><u>embeddedness, decision theory, logical counterfactuals</u></a><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/BSpdshJWGAW6TuNzZ/introduction-to-cartesian-frames\"><u>Cartesian Frames</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "kF4Yucz9kz",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Evan Hubinger (2022)<br>",
                "<div>Employment:</div><div><span><span><span><a href=\"https://www.anthropic.com/\"><u>Anthropic</u></a><br></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span><span><span><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem\"><u>Deception + Inner Alignment</u></a><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/GeabLEXYP7oBMivmF/acceptability-verification-a-research-agenda\"><u>Evan's research agenda</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span><a href=\"https://www.google.com/url?q=https://docs.google.com/presentation/d/1IzmmUSvhjeGhc_nc8Wd7-hB9_rSeES8JvEvKzQ8uHBI/edit?usp%3Ddrivesdk&amp;sa=D&amp;source=editors&amp;ust=1661633213188468&amp;usg=AOvVaw1-ALhgrpPnw_4Y0uRozVl_\"><u>Deceptive agents are the default</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "QN0d~Ot9Z:",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Rohin Shah (2022)<br>",
                "<div>Employment:</div><div><div><span><span><span><a href=\"https://www.deepmind.com/\"><u>DeepMind</u></a></span></span></span></div><a href=\"https://www.anthropic.com/\"></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div>produces the <span><span><span><a href=\"https://rohinshah.com/alignment-newsletter/\"><u>alignment newsletter</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "fccY2Ph3<e",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - IBP<br>",
                "<span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized\"><u>Infra-Bayesian Physicalism</u></a></span></span>"
            ],
            "guid": "BytPdJ{6KZ",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Vanessa Kosoy (2022)<br>",
                "<div>Employment:<span><span><span><br></span></span></span></div><div><span><span><span><a href=\"https://intelligence.org/\"><u>MIRI</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div>Vanessa's<span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda\"> research agenda</a><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized\"><u>Infra-Bayesian Physicalism</u></a></span></span>&nbsp;(IBP)</div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "u;NOM]EbU~",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - IB<br>",
                "<span><span><a class=\"SequencePreview-link\" id=\"CmrW8fCmSLK7E25sa\" href=\"https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa\"><u>Infra-Bayesianism</u></a></span></span><br>"
            ],
            "guid": "AOe)&g^t&D",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Tamera Lanham (2022)<br>",
                "<div>Employment:</div><div>INCOMPLETE</div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/FRRb6Gqem8k69ocbi/externalized-reasoning-oversight-a-research-direction-for\"><u>Externalized Reasoning Oversight</u></a></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "pjI![OBYpf",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - FAR<br>",
                "<div><span><span><span><a href=\"https://alignmentfund.org/\">Fund For Alignment Research<br></a></span></span></span></div>"
            ],
            "guid": "t)1u@4?@7U",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - FHI<br>",
                "<div><span><span><span><a href=\"https://www.fhi.ox.ac.uk/\"><u>Future of Humanity Institute</u></a></span></span></span><br></div>"
            ],
            "guid": "kVqjY%QwU8",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - HCH<br>",
                "<span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch\">Human Consulting Human</a></span></span>"
            ],
            "guid": "deYUuBXsdm",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - RLHF",
                "Reinforcement Learning from Human Feedback<br>"
            ],
            "guid": "cvjs075XpQ",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about John Wentworth (2022)<br>",
                "<div>Employment:</div><div>INCOMPLETE</div><div><br></div><div><span><span><span>Working on:</span></span></span><span><span><br></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/3L46WGauGpr7nYubu/the-plan\"><u>Selection Theorems</u></a><span>:</span></span></span> try to figure out what types of agents are selected for in a broad range of environments</div><div><br></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "mKx>N7WQmk",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Owain Evens (2022)<br>",
                "<div>Employment:</div><div><a href=\"https://www.anthropic.com/\"><span><span><span></span></span></span></a><a href=\"https://truthful.ai/\"><u>Truthful AI</u></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi\"><u>Truthful LMs as a warm-up for aligned AGI</u></a></span></span> <br></div><div><span><span><span><a href=\"https://arxiv.org/abs/2109.07958\"><u>TruthfulQA: Measuring How Models Mimic Human Falsehoods</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "ildrxr5Jfm",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Owen Cotton-Barratt (2022)<br>",
                "<div>Employment:</div><div><a href=\"https://www.anthropic.com/\"><span><span><span></span></span></span></a><a href=\"https://truthful.ai/\"><u>Truthful AI</u></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div><div><span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi\"><u>Truthful LMs as a warm-up for aligned AGI</u></a></span></span> <br></div><div><span><span><span><a href=\"https://arxiv.org/abs/2109.07958\"><u>TruthfulQA: Measuring How Models Mimic Human Falsehoods</u></a></span></span></span></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "p**P|-<aKL",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - OOD<br>",
                "Out of Distribution<br>"
            ],
            "guid": "cp][/.YC=Y",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Acronym"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Neuron Monosemanticity?<br>",
                "Making a neuron represent only one thing, as opposed to firing on many unrelated concepts"
            ],
            "guid": "x~q%M`{i[j",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Definition"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Geoffrey Irving (2022)<br>",
                "<div>Employment:<span><span><br></span></span></div><div><a class=\"TagHoverPreview-link\" href=\"https://www.lesswrong.com/tag/agent-foundations\"><span><span><span></span></span></span></a><a href=\"https://www.deepmind.com/\"><u>DeepMind</u></a></div><div><span><span><span><br></span></span></span></div><div><span><span><span>Working on:</span></span></span></div>debate as an alignment strategy: more <span><span><a class=\"PostLinkPreviewWithPost-link\" href=\"https://www.lesswrong.com/posts/bLr68nrLSwgzqLpzu/axrp-episode-16-preparing-for-debate-ai-with-geoffrey-irving\"><u>detail here</u></a></span></span>.<div><span><span><span><br></span></span></span></div><div><span><span><span>Views:</span></span></span></div><div><span><span><span>INCOMPLETE<br></span></span></span></div><div><span><span><span><br></span></span></span></div><span><span><span>(needs source)</span></span></span>"
            ],
            "guid": "klmjAl0#z2",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": [
                "Individual"
            ]
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Phase Change duing training?",
                "A Phase Change is when a neural net abruptly shifts during training to a more effective solution.<br>E.g. training on numerical addition shifts from memorization to generalization at some point in the training and becomes much more accurate at that point.<br><a href=\"https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\">See: Grokking</a>"
            ],
            "guid": "zO&.tSxos^",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is an Induction Head?",
                "Induction heads are implemented by a circuit consisting of a pair of \nattention heads in different layers that work together to copy or \ncomplete patterns. The first attention head copies information from the \nprevious token into each token. This makes it possible for the second \nattention head to attend to tokens based on what happened before them, \nrather than their own content.<br>See: <a href=\"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#definition-of-induction-heads\">Induction Heads</a>&nbsp;or&nbsp;<a href=\"https://transformer-circuits.pub/2021/framework/index.html#induction-heads\">This Short Section</a>"
            ],
            "guid": "J7Hmn)Joiy",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Weight Decay?",
                "Weight Decay is a regularization technique in deep learning by adding a penalty term to the cost function which has the effect of shrinking the weights during backpropagation.<br>This helps prevent the network from overfitting the training data as well as the exploding gradient problem.<br>\\(w_{new} = w_{old} - \\alpha (\\frac{\\partial w}{\\partial E}+\\lambda w_{old})\\)<br><a href=\"https://programmathically.com/weight-decay-in-neural-networks/\">Weight Decay</a><br>"
            ],
            "guid": "CU=38g)dfm",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the Orthogonality Thesis?",
                "The Orthogonality Thesis asserts that there can exist arbitrarily intelligent agents pursuing any kind of goal.<br><br>The strong form of the Orthogonality Thesis says that there's no extra \ndifficulty or complication in creating an intelligent agent to pursue a \ngoal, above and beyond the computational tractability of that goal.<br><br><a href=\"https://arbital.com/p/orthogonality/\">https://arbital.com/p/orthogonality/</a>"
            ],
            "guid": "_o&G9f#<2",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Eliezer Yudkowsky (2022)",
                "<div>Employment:</div><div>INCOMPLETE</div><div><br></div><div>Working on:</div><div>INCOMPLETE</div><div><br></div><div>Views:</div><div><a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">source</a>&nbsp;for these:<br></div><div>- if you can get as incredibly far as \"less than roughly certain to kill everybody\", then you can probably get down to under a 5% chance with only slightly more effort</div><div>- \"AGI will not be upper-bounded by human ability or human learning speed\"</div><div>- \"<strong>A cognitive system with \nsufficiently high cognitive powers, given any medium-bandwidth channel \nof causal influence, will not find it difficult to bootstrap to \noverpowering capabilities independent of human infrastructure\"</strong></div><div><strong>- \"</strong><strong>We need to get alignment right on the 'first critical try'\"</strong></div><div><strong>- \"</strong><strong>We need to align the \nperformance of some large task, a 'pivotal act' that prevents other \npeople from building an unaligned AGI that destroys the world.\"</strong></div><div><strong>- \"</strong><strong>Many alignment problems of superintelligence will not naturally appear at pre-dangerous, passively-safe levels of capability\"</strong></div><div><strong>- \"</strong><strong>Fast capability gains seem likely, and may break lots of previous alignment-required invariants simultaneously\"</strong></div><div><strong>- \"</strong><strong>outer optimization even on a very exact, very simple loss function doesn't produce inner optimization in that direction\"</strong></div><div><strong>- \"</strong><strong>Corrigibility is anti-natural to consequentialist reasoning\"</strong></div><div><strong>- \"</strong><strong>Any system of sufficiently \nintelligent agents can probably behave as a single agent, even if you \nimagine you're playing them against each other\"</strong></div><div><strong>-&nbsp;</strong></div> <div><br></div><br>"
            ],
            "guid": "Kco/G#?bTd",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Inner Alignment?",
                "Inner alignment is the degree to which the interal (typically thought of as inferred) utility function maps onto the explit utility function the agent is trained on.<br><br>A trained agent will have an approximate respresentation of the explicit utility function, which may not map exactly on that actual explicit utility function. For example, the utility function that evolution trains organisms on is generally considered \"genetic fitness\" or \"gene propagation\", yet humans (who were developed by evolution) usually don't explicitly carry the goal of propagating their genes. Instead they express proximate goals such as seeking power, influence, and happiness, which may or may not directly lead to gene propagation."
            ],
            "guid": "CPv%?*#HI)",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Outer Alignment?",
                "Outer Alignment is the degree to which an agent's actions maximize the utility function they are trained on.<br><br>INCOMPLETE"
            ],
            "guid": "Qe&`LFf4)!",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is the AI Alignment Problem?",
                "Ensure that AI's values and actions are aligned with human values<br>Most smart agents may be misaligned by default because they may compete with us for power and resources.<br><br><a href=\"https://neurips.cc/virtual/2021/poster/28400\">Optimal Policies Tend To Seek Power</a>"
            ],
            "guid": "fBK2!0PTMA",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Buck Shlegeris (2022)",
                "Employment:<br>CTO of Redwood Research<br><br>Working on:<br>EA movement building<br>...<br><br>Views:<br>- Worst case thinking is likely a beneficial attitude&nbsp;<a href=\"https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment\">https://www.lesswrong.com/posts/yTvBSFrXhZfL8vr5a/worst-case-thinking-in-ai-alignment</a><br>"
            ],
            "guid": "zGJLvE%3I~",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Facts about Nate Soares (2022)",
                "Employment:<br>Executive Director at MIRI<br><br>Working on:<br><br><br>Views:<br>- \"the hard bits are much more technical than moral\"&nbsp;<a href=\"https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization\">source</a><br>-&nbsp;<br><br><br>"
            ],
            "guid": "fB;(hx&MoV",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Goal Misgeneralization?",
                "Goal Misgeneralization is when an agent is capable, but optimizing for the wrong reward during distributional shift.<br>E.g. a bot is trained to collect a coin, but the coin is always in the same place during training. The, once deployed and the coin has moved, the bot just goes to the place where the coin usually is. The bot is acting capably, but has not generalized the goal properly.<br><a href=\"https://arxiv.org/abs/2105.14111\">source</a>"
            ],
            "guid": "Q/8.Vu|Fa,",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - CID",
                "CID is Causal Inference Diagram <a href=\"https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents\">source</a>"
            ],
            "guid": "xV#_RuFl47",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a Causal Inference Diagram (CID)?",
                "A Causal Inference Diagram is a unified theory of how design decisions create incentives that shape agent behavior.<br>A CID represents decisions, states, and utilities of a markov decision process in order to formalize the causal chain of events.<br>It takes the form of a flowchart where Decisions are generally squares, States are circles or rounded squares, and Utilities are diamonds. Different subgraphs can be colored differently to represent connections between individual actors and their utilities.<br><a href=\"https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1#b09d\">source</a><br><br>For explansion on CID, please see&nbsp;<a href=\"https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents\">Mechanised Causal Graphs</a>&nbsp;which expands on this model."
            ],
            "guid": "j!42h2f_v_",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a Mechanised Causal Graph?",
                "A Mechanised Causal Graph is a variant of&nbsp; <a href=\"https://drive.google.com/file/d/1_OBLw9u29FrqROsLfhO6rIaWGK4xJ3il/view\">mechanised causal game</a> graph used to facilitate better formalization of Causal Inference Diagrams (CID).<br>The Mechanised Causal Graph adds mechanism nodes to each node in the CID (generally denoted by a new node with a tilda above a letter matching the CID node the mechanism node is attached to) and directed edges (generally dotted lines while the original CID edges are solid) which depict the mechanistic dependencies. These mechanistic dependencies are of 2 types: terminal and non-terminal. Terminal dependencies are dependencies which connect utilities to their decisions directly, while non-terminal dependencies are dependencies which only exist insofar as they effect the causal path towards the utility.<br><br>For instance, take the following CID:<br>A[decision] -&gt; B[state] -&gt; C[utility]<br>Where A and C are colored the same implying that C is the utility resultant from decision A.<br>The Mechanised Causal Graph would be the following:<br>A' &lt;- - B'&nbsp; &nbsp; &nbsp; &nbsp;C' - - - &gt; A' (this A' is same as A' earlier on this line, this is just simpler)<br>|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |<br>A ----&gt; B ---&gt; C<br>The dotted line C' - - - &gt; A' is a terminal edge because C is the payoff to A, and no matter what will always be a causal factor in A, thus attached to A'<br>Yet dotted line B' - - - &gt; A' is not terminal, because if the solid line B ---&gt; C is severed, A' no longer needs to be informed by B. This is equivalent to saying that decision A only cares about state B insofar as it effects payout C. Decision A will always care about payout C, thus that mechanistic dependencies is terminal, even though the mechanistic dependency between A and B is not.<br><br><a href=\"https://www.lesswrong.com/posts/XxX2CAoFskuQNkBDy/discovering-agents\">source</a>"
            ],
            "guid": "zA2ol5yZcf",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a Treacherous Turn?",
                "A Treacherous Turn is a situation where training produces a consequentialist agent sophisticated enough to understand that it needs to perform well during training in order to avoid a negative reward. After deployment, when the agent is safe from futher negative rewards, it no longer acts in a restrained manner.<br>(coined by Nick Bostrom in Superintelligence: Chapter 8)<br><br><a href=\"https://www.lesswrong.com/posts/B39GNTsN3HocW8KFo/superintelligence-11-the-treacherous-turn\">source</a><br><br>"
            ],
            "guid": "Mrzn_m9][z",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - SDP",
                "SDP means semidefinite program"
            ],
            "guid": "JqQi=^VjN5",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acrynym - L-BFGS",
                "L-BFGS (or LM-BFGS) is Limited-memory BFGS -&nbsp;<a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">Broyden–Fletcher–Goldfarb–Shanno algorithm</a>"
            ],
            "guid": "fo;~:y5s`3",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Limited-memory BFGS?",
                "L-BFGS is a limited-memory version of the&nbsp;&nbsp;<a href=\"https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">Broyden–Fletcher–Goldfarb–Shanno algorithm</a>&nbsp;(which uses arbitraty computer memory). It is used to find an input x that minimizes a networks output f(x).<br><a href=\"https://en.wikipedia.org/wiki/Limited-memory_BFGS\">source</a>"
            ],
            "guid": "E-QZ5863`s",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - AC-GAN",
                "AC-GAN is Auxiliary Classifier Generative Adversarial Network"
            ],
            "guid": "bbb]CDj1Ia",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(L_0\\),&nbsp;\\(L_1\\),&nbsp;\\(L_2\\),&nbsp;\\(\\dots\\),&nbsp;\\(L_\\infty\\)&nbsp;norms",
                "L-norms measure a type of length. In machine learning, generally applied to a vector.<br>For example set vector&nbsp;\\({\\bf X} = \\{0, 1, 2, {-3}\\}\\)<br>Then the&nbsp;\\(L_n\\)&nbsp;norm is definied as follows (let&nbsp;\\(0^0=0\\)&nbsp;for calculation purposes):<br>\\(L_n = ||{\\bf X}||_n = \\sqrt[n]{|0|^n+|1|^n+|2|^n+|{-3}|^n}\\)<br>\\(L_0\\)&nbsp;norm:&nbsp;\\(||{\\bf X}||_0 = 0+1+1+1 = 3\\), which is the number of non-zero elements in the vector<br>\\(L_1\\)&nbsp;norm:&nbsp;\\(||{\\bf X}||_1 = 0+1+2+3 = 6\\), which is the sum of magnitudes of elements in the vector<br>\\(L_2\\)&nbsp;norm:&nbsp;\\(||{\\bf X}||_2 = \\sqrt{0+1+4+9} = \\sqrt{15}\\), which is the square-root of the sum of squares of elements in the vector. This is the most common norm. If there is no subscript, this is most likely what is indicated by&nbsp;\\(||{\\bf X}||\\). It is also called the euclidian distance (think of calculating the length of the hypotenuse of a right triangle)<br>\\(L_\\infty\\)&nbsp;norm:&nbsp;\\(||{\\bf X}||_\\infty = max(\\{|0|, |1|, |2|, |-3|\\}) = 3\\), which is the magnitude of the largest element of&nbsp;\\({\\bf X}\\).<br><a href=\"https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\">source</a>"
            ],
            "guid": "MejZ9Sf$5)",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - GAN",
                "Generative Adversarial Network"
            ],
            "guid": "vqyYeOr}6[",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is a Generative Adversarial Network (GAN)?",
                "A GAN is a methodology to create a discriminator that reliably detects fake images and a generator that makes convincingly real images. Thus, a GAN is composed of a generator and descriminator. The generator maps noise to a synthetic image. While the discriminator receives the image and distinguishes if the image is real or generated.<br><a href=\"https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/\">source</a>"
            ],
            "guid": "HBC_zphUT~",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(argmax\\)",
                "The operator argmax returns the value in some domain that gives the largest evaluation to the arguments passed to argmax. The variable or domain that argmax searchers over is generally placed directly under&nbsp;\\(argmax\\).<br>Example:<br>let&nbsp;\\(a \\in A\\)<br>some function&nbsp;\\(F: A \\rightarrow \\mathbb{R}\\)<br>\\(\\underset{a}{argmax}(F(a)) = a^*\\)<br>Thus,&nbsp;\\(a^* \\in A\\)&nbsp;that returns the largest value when passed to&nbsp;\\(F\\)<br>Sometimes, the same equation will be notated:<br>\\(\\underset{A}{argmax}(F) = a^*\\)<br>in the case that it is obvious that&nbsp;\\(F\\)&nbsp;is a function that receives arguments from&nbsp;\\(A\\)."
            ],
            "guid": "t-/5A+GS=K",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(\\theta\\)&nbsp;(theta)",
                "In machine learning,&nbsp;\\(\\theta\\)&nbsp;generally refers to the set of parameters in a model. For a convolutional neural network,&nbsp;\\(\\theta\\)&nbsp;is the set of weights and biases."
            ],
            "guid": "p3nkl`.uTC",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Acronym - ReLU",
                "Rectified Linear Unit"
            ],
            "guid": "5:zhzHtQI",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(ReLU\\)&nbsp;(rectified linear unit)",
                "\\(ReLU(x) = max(0, x)\\)<br>Let&nbsp;\\({\\bf X} = \\{x_0, x_1, \\dots, x_n\\}\\)<br>\\(ReLU({\\bf X}) = \\{ ReLU(x_0), ReLU(x_1), \\dots, ReLU(x_n)\\}\\)"
            ],
            "guid": "EOa>NWO]2-",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(F({\\bf X})\\)&nbsp;(applying&nbsp;\\(F\\)&nbsp;to&nbsp;a vector/matrix)",
                "Applying a function&nbsp;\\(F\\)&nbsp;to a matrix is different depending on the function.<br>If&nbsp;\\(F\\)&nbsp;is a function from&nbsp;\\(\\mathbb{R} \\rightarrow \\mathbb{R}\\), then applying&nbsp;\\(F\\)&nbsp;to a matrix would just apply&nbsp;\\(F\\)&nbsp;element-wise.<br>But if&nbsp;\\(F\\)&nbsp;is a function from&nbsp;\\(\\mathbb{R}^{n\\times m} \\rightarrow \\mathbb{R}\\), then&nbsp;\\(F\\)&nbsp;will be applied to the matrix as a whole.<br>In general, it's important to keep in mind what the domain of each function is when analysizing, but understand that functions may be applied element-wise when it is a function from the domain of reals."
            ],
            "guid": "GX|>PFKK3S",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(\\Bbb E\\)",
                "\\(\\Bbb E\\)&nbsp;represents expected value.<br>It is a function which applies to an expression usually over some domain and it represents what is the mean return of the expression when evaluated across the given domain.<br>\\(\\Bbb{E}_{a\\in A}(F(a))\\)&nbsp;is generally the unweighted average return of&nbsp;\\(F\\)&nbsp;when given all the different elements of&nbsp;\\(A\\)<br>\\(\\Bbb E\\)&nbsp;can be evaluated over multiple domains simultaneously:<br>\\(\\Bbb{E}_{a\\in A, b\\in B}(F(a, b))\\)<br>\\(\\Bbb{E}\\)&nbsp;can be evaluated non-uniformly over some domain. Let&nbsp;\\(\\rho\\)&nbsp;be a probability distribution.<br>\\(\\Bbb{E}_{x\\sim\\rho}(F(x))\\)&nbsp;is the average return of&nbsp;\\(F\\)&nbsp;when given all different values of&nbsp;\\(x\\)&nbsp;and weighted by its probability in&nbsp;\\(\\rho\\)<br>"
            ],
            "guid": "y6t_&n3pCT",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Notation -&nbsp;\\(\\varpropto\\)",
                "\\(x \\varpropto y\\)&nbsp;means&nbsp;\\(x\\)&nbsp;is proportional to&nbsp;\\(y\\)"
            ],
            "guid": "y?PI`*]/N`",
            "note_model_uuid": "03abfc49-4679-11ed-ad40-00248c528cf7",
            "tags": []
        }
    ],
    "reviewLimit": null,
    "reviewLimitToday": null
}
